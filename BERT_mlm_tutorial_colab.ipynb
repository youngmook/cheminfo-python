{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT_mlm_tutorial_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngmook/cheminfo-python/blob/main/BERT_mlm_tutorial_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPtOGr8T_wxw"
      },
      "source": [
        "# BERT(masked language model) tutorial\n",
        "\n",
        "[Reference URL](https://keras.io/examples/nlp/masked_language_modeling/)\n",
        "\n",
        "BERT 학습 및 분류 fine-tuning 실습코드 리뷰\n",
        "\n",
        "220516\n",
        "\n",
        "고우영"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG_O96CrxR0u"
      },
      "source": [
        "# End-to-end Masked Language Modeling with BERT\n",
        "\n",
        "**Author:** [Ankur Singh](https://twitter.com/ankur310794)<br>\n",
        "**Date created:** 2020/09/18<br>\n",
        "**Last modified:** 2020/09/18<br>\n",
        "**Description:** Implement a Masked Language Model (MLM) with BERT and fine-tune it on the IMDB Reviews dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXPQ005xxR0x"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Masked Language Modeling is a fill-in-the-blank task,\n",
        "where a model uses the context words surrounding a mask token to try to predict what the\n",
        "masked word should be.\n",
        "\n",
        "For an input that contains one or more mask tokens,\n",
        "the model will generate the most likely substitution for each.\n",
        "\n",
        "Example:\n",
        "\n",
        "- Input: \"I have watched this [MASK] and it was awesome.\"\n",
        "- Output: \"I have watched this movie and it was awesome.\"\n",
        "\n",
        "Masked language modeling is a great way to train a language\n",
        "model in a self-supervised setting (without human-annotated labels).\n",
        "Such a model can then be fine-tuned to accomplish various supervised\n",
        "NLP tasks.\n",
        "\n",
        "This example teaches you how to build a BERT model from scratch,\n",
        "train it with the masked language modeling task,\n",
        "and then fine-tune this model on a sentiment classification task.\n",
        "\n",
        "We will use the Keras `TextVectorization` and `MultiHeadAttention` layers\n",
        "to create a BERT Transformer-Encoder network architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCoNAnhzxR0y"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install `tensorflow 2.8.0` via `pip install tensorflow==2.8.0`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtKzdDcB1X8z",
        "outputId": "1426da3d-a6ba-4c66-b18b-e1e2abe791c7"
      },
      "source": [
        "## 버전 확인\n",
        "import tensorflow as tf\n",
        "print('TF version : %s\\n'%tf.__version__)\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version : 2.8.0\n",
            "\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 5760430669341043960\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11320098816\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 10069362004369244149\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woa9GWK6xR1C"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.keras.layers.multi_head_attention import MultiHeadAttention\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YINur7JPxR1E"
      },
      "source": [
        "## Set-up Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiKYJba8xR1F"
      },
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 32\n",
        "    LR = 0.001\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEAD = 8  # used in bert model\n",
        "    FF_DIM = 128  # used in bert model\n",
        "    NUM_LAYERS = 1\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd6_NJaNxR1G"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "We will first download the IMDB data and load into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEeVQ22oxR1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8007a02-19b8-4ad1-c421-a2e3ba910a83"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  26.0M      0  0:00:03  0:00:03 --:--:-- 26.0M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa3w9jfuxR1H",
        "outputId": "8015cd98-d425-47a8-f20d-5c693a36b10c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "\n",
        "def get_text_list_from_files(files):\n",
        "    text_list = []\n",
        "    for name in files:\n",
        "        with open(name, encoding='UTF-8') as f:\n",
        "            for line in f:\n",
        "                text_list.append(line)\n",
        "                \n",
        "                if len(text_list) > 256:  break\n",
        "                    \n",
        "    return text_list\n",
        "\n",
        "\n",
        "def get_data_from_text_files(folder_name):\n",
        "\n",
        "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
        "    pos_texts = get_text_list_from_files(pos_files)\n",
        "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
        "    neg_texts = get_text_list_from_files(neg_files)\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"review\": pos_texts + neg_texts,\n",
        "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
        "        }\n",
        "    )\n",
        "    df = df.sample(len(df)).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = get_data_from_text_files(\"train\")\n",
        "test_df = get_data_from_text_files(\"test\")\n",
        "\n",
        "all_data = train_df.append(test_df)\n",
        "all_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  When I was very young,on a local tv station,th...          0\n",
              "1  Romance is in the air and love is in bloom in ...          0\n",
              "2  Having loved 'Paris, Je T'aime', I highly anti...          1\n",
              "3  To call a film about a crippled ghost taking r...          1\n",
              "4  Twelve years ago, production stopped on the sl...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47496a36-43d4-46a5-a1aa-582254fe8c24\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When I was very young,on a local tv station,th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Romance is in the air and love is in bloom in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Having loved 'Paris, Je T'aime', I highly anti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To call a film about a crippled ghost taking r...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Twelve years ago, production stopped on the sl...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47496a36-43d4-46a5-a1aa-582254fe8c24')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47496a36-43d4-46a5-a1aa-582254fe8c24 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47496a36-43d4-46a5-a1aa-582254fe8c24');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Kza8CKxR1I"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We will use the `TextVectorization` layer to vectorize the text into integer token ids.\n",
        "It transforms a batch of strings into either\n",
        "a sequence of token indices (one sample = 1D array of integer token indices, in order)\n",
        "or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n",
        "\n",
        "Below, we define 3 preprocessing functions.\n",
        "\n",
        "1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n",
        "2.  The `encode` function encodes raw text into integer token ids.\n",
        "3.  The `get_masked_input_and_labels` function will mask input token ids.\n",
        "It masks 15% of all input tokens in each sequence at random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          1,
          8
        ],
        "id": "J0oRnP1g_cKY",
        "outputId": "d78c137e-f6d2-4d40-8e49-4c23b64c1c1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)  # 소문자화\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")  # html 제거\n",
        "    # 특수문자 제거\n",
        "    return tf.strings.regex_replace(stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\")\n",
        "\n",
        "\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "    \"\"\"Build Text vectorization layer\n",
        "\n",
        "    Args:\n",
        "      texts (list): List of string i.e input texts\n",
        "      vocab_size (int): vocab size\n",
        "      max_seq (int): Maximum sequence lenght.\n",
        "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Return TextVectorization Keras Layer\n",
        "    \"\"\"\n",
        "    # 텍스트를 벡터화 하는 함수\n",
        "    # ex) ['i have a dream'] -> [10, 25, 4, 1040]\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        standardize=custom_standardization,  # 따로 정의한 정제함수\n",
        "        output_sequence_length=max_seq,\n",
        "    )\n",
        "    vectorize_layer.adapt(texts)  # texts 적용\n",
        "\n",
        "    # Insert mask token in vocabulary\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]  # [mask] token 추가\n",
        "    vectorize_layer.set_vocabulary(vocab)\n",
        "    return vectorize_layer\n",
        "\n",
        "\n",
        "vectorize_layer = get_vectorize_layer(all_data.review.values.tolist(),\n",
        "                                      config.VOCAB_SIZE,\n",
        "                                      config.MAX_LEN,\n",
        "                                      special_tokens=[\"[mask]\"])\n",
        "\n",
        "# Get mask token id for masked language model\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
        "print('[mask] token id : %s'%mask_token_id)\n",
        "print(vectorize_layer(['i have a dream']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[mask] token id : 29999\n",
            "tf.Tensor(\n",
            "[[  10   25    4 1040    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]], shape=(1, 256), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          2,
          7
        ],
        "id": "OfmR3DVb_cKY"
      },
      "source": [
        "# texts -> int로 벡터화\n",
        "# ['i have a dream'] -> tf.Tensor([[10, 25, 4, 1040, 0]])\n",
        "def encode(texts):\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    return encoded_texts.numpy()\n",
        "\n",
        "\n",
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    # input(encoded_texts) : array([[10, 25, 4, 1040, 0, ..], [])\n",
        "    # output 1), masking된 입력\n",
        "    # encoded_texts_masked = [[    5 29999    10 22697    25]\n",
        "    #                         [   56    11   277    28 29999]\n",
        "    #                         [    1  1596     3     2   188]]    \n",
        "    # output 2), 80%의 masking, random exchange, 원래단어 표시\n",
        "    # y_labels = [[    5            29999(mask)    10      22697(랜덤교체단어)    25]\n",
        "    #             [   56(원래단어)    11            277     28                     29999(mask)]\n",
        "    #             [    1              1596          3       2                     188]]\n",
        "    # output 3), 계산할 부분만 1로\n",
        "    # sample_weights = [[0. 1. 0. 1. 0.]\n",
        "    #                   [1. 0. 0. 0. 1.]\n",
        "    #                   [0. 0. 0. 0. 0.]]\n",
        "    \n",
        "    \n",
        "    ## 15%는 masking 한다\n",
        "    # 1. 80%는 단순 masking\n",
        "    # 2. 10%는 다시 원래단어로 복구\n",
        "    # 3. 10%는 아무 단어로 변형\n",
        "    \n",
        "    # 15% BERT masking\n",
        "    # encoded_texts : (N, seq_len)\n",
        "    # np.random.rand(*encoded_texts.shape) : (N, seq_len) 의 0~1로 이루어진 행렬\n",
        "    # inp_mask : 0.15보다 작으면 True, 크면 False로 이루어진 (N, seq_len) 행렬, array([[False, False, True, False, ..], [])\n",
        "    # inp_mask : array([[False, False, False, False, False],\n",
        "    #                   [False,  True, False, False, False],\n",
        "    #                   [False, False, False, False,  True]])\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15  \n",
        "    # Do not mask special tokens\n",
        "    # inp_mask = [[False, True,  False, True,  False]\n",
        "    #             [True,  False, False, False, True]\n",
        "    #             [False, False, False, False, False]]\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    # labels : array([[-1, -1, -1, -1, -1],\n",
        "    #                 [-1, -1, -1, -1, -1],\n",
        "    #                 [-1, -1, -1, -1, -1]])\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # Set labels for masked tokens\n",
        "    # labels : array([[  -1,   259,   -1,   57,   -1],\n",
        "    #                 [  56,   -1,   -1,   -1,    5],\n",
        "    #                 [  -1,   -1,   -1,   -1,    -1]])\n",
        "    \n",
        "    ## 1. 80%는 단순 masking\n",
        "    #labels = [[-1  259  -1   57   -1]\n",
        "    #          [56  -1   -1   -1   5]\n",
        "    #          [-1  -1   -1   -1   -1]]\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    ## 2. 10%는 다시 원래단어로 복구\n",
        "    # Prepare input\n",
        "    # encoded_texts_masked = [[   5  259   10   57   25]\n",
        "    #                         [  56   11  277   28    5]\n",
        "    #                         [   1 1596    3    2  188]]\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
        "    # This means leaving 10% unchanged\n",
        "    #inp_mask_2mask = [[False  True False  True False]\n",
        "    #                  [False False False False  True]\n",
        "    #                  [False False False False False]]\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    # encoded_texts_masked = [[    5 29999    10 29999    25]\n",
        "    #                          [   56    11   277    28 29999]\n",
        "    #                          [    1  1596     3     2   188]]\n",
        "    encoded_texts_masked[inp_mask_2mask] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "    ## 3. 10%는 아무 단어로 변형\n",
        "    # Set 10% to a random token\n",
        "    # inp_mask_2random = [[False False False  True False]\n",
        "    #                     [False False False False False]\n",
        "    #                     [False False False False False]]\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    # encoded_texts_masked = [[    5 29999    10 22697    25]\n",
        "    #                         [   56    11   277    28 29999]\n",
        "    #                         [    1  1596     3     2   188]]\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(3, mask_token_id, inp_mask_2random.sum())\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    # 계산할 부분은 1, 나머지 부분은 0, 15%에 해당하는 부분만 표시, 80%는 mask, 10%는 원래단어, 10%는 랜덤교체단어\n",
        "    # [[0. 1. 0. 1. 0.]\n",
        "    #  [1. 0. 0. 0. 1.]\n",
        "    #  [0. 0. 0. 0. 0.]]\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    # [[    5            29999(mask)    10      22697(랜덤교체단어)    25]\n",
        "    # [   56(원래단어)    11            277     28                     29999(mask)]\n",
        "    # [    1              1596          3       2                     188]]\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "    \n",
        "    \n",
        "    return encoded_texts_masked, y_labels, sample_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgYfoilQxR1I",
        "outputId": "a1630a62-3c41-4290-bcce-29fac8df7fb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## prepair fine-tuning data\n",
        "# We have 25000 examples for training\n",
        "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
        "print('x_train_before : %s\\n'%train_df.review.values[0])\n",
        "print('x_train_after  : %s\\n'%x_train[0])\n",
        "\n",
        "# 학습데이터의 label(긍정or부정 정보)\n",
        "y_train = train_df.sentiment.values\n",
        "train_classifier_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(1000)\n",
        "    .batch(config.BATCH_SIZE)\n",
        ")\n",
        "print('\\ny_train plot : %s'%y_train[:5])\n",
        "\n",
        "# We have 25000 examples for testing\n",
        "x_test = encode(test_df.review.values)\n",
        "y_test = test_df.sentiment.values  # 테스트 데이터의 label(긍정or부정 정보)\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(config.BATCH_SIZE)\n",
        "\n",
        "# Build dataset for end to end model input (will be used at the end)\n",
        "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices((test_df.review.values, y_test)).batch(config.BATCH_SIZE)\n",
        "\n",
        "#######################################################\n",
        "## Prepare data for masked language model\n",
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(x_all_review)\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_train, y_masked_labels, sample_weights))\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)\n",
        "print('data preparation done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train_before : When I was very young,on a local tv station,they would show kung fu movies of all kinds on Saturdays.I saw lots of Kung Fu movies on weekends.I remember lots of them.I saw great flicks like Crippled Masters,Blind Fist of Bruce,Kung Fu Zombie,Shaolin Drunken Monk,Rage of the Master,Tattoe Dragon,and...Five Deadly Venoms.I remember the day clearly.Me and my dad had just gotten lunch at Burger King.We were racing home to see what movie it would be this saturday.We ran in the house and jumped onto the couch,turned on the set and flicked it onto 56.The usual intro of many kung fu movie clips in the background with the words Kung Fu Saturday over it.Then under that was the Title of the film.It said Five Deadly Venoms.Then the movie began.I bit into my burger amused with the pre-credit sequence.I loved this movie the minute it came on.My favorite character was the Toad Venom.The plot was hard to follow at that age but that wasn't what lured me...it was the fighting.The fights were so...amazing.I moaned every time a commercial came on and soon the 2 hours of the best movie i have ever seen ended.\n",
            "\n",
            "x_train_after  : [   50    10    13    52     1     4   681   241     1    57   118  3067\n",
            "  2804    92     5    31  2662    20     1   202   711     5  3067  2804\n",
            "    92    20     1   372   711     5     1   202    79  1430    38  6744\n",
            "     1  5736     5     1  2804     1  3894     1     5     2     1     1\n",
            "  2504     1   372     2   267     1     3    54  1360    67    40  1830\n",
            "  5989    30 15498     1    66  5646   348     6    64    48    17     9\n",
            "    57    26    11     1  2040     8     2   339     3  5047  1526     2\n",
            "     1    20     2   284     3 27272     9  1526     1   644  6374     5\n",
            "   104  3067  2804    17  2990     8     2   928    15     2   659  3067\n",
            "  2804  2418   128     1   488    12    13     2   412     5     2 13934\n",
            "   295   753  2504     1     2    17     1   222    80    54 15498  5308\n",
            "    15     2     1     1   418    11    17     2   883     9   367     1\n",
            "   502   106    13     2 18299     1   112    13   263     6   846    30\n",
            "    12   638    18    12   277    48 11290 26976    13     2     1  1823\n",
            "    66     1     1   166    59     4  2072   367    20     3   521     2\n",
            "   298   616     5     2   115    17    10    25   120   105  1006     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "\n",
            "\n",
            "y_train plot : [0 0 1 1 0]\n",
            "data preparation done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLWD_MIq_cKf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrinedex_cKg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0bA06tuxR1O"
      },
      "source": [
        "## Create BERT model (Pretraining Model) for masked language modeling\n",
        "\n",
        "We will create a BERT-like pretraining model architecture\n",
        "using the `MultiHeadAttention` layer.\n",
        "It will take token ids as inputs (including masked tokens)\n",
        "and it will predict the correct ids for the masked input tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4npVZ5_xR1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0797e39f-00eb-4b2c-ad96-034e79333144"
      },
      "source": [
        "\n",
        "def Transformer_block(query, key, value, i):\n",
        "    # Multi headed self-attention\n",
        "    attention_output = layers.MultiHeadAttention(num_heads=config.NUM_HEAD,\n",
        "                                                 key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "                                                 name=\"encoder_{}/multiheadattention\".format(i),\n",
        "                                                 )(query, key, value)\n",
        "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
        "        attention_output)\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i))(\n",
        "        query + attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ffn = keras.Sequential([layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "                            layers.Dense(config.EMBED_DIM)],\n",
        "                            name=\"encoder_{}/ffn\".format(i))\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(ffn_output)\n",
        "    sequence_output = layers.LayerNormalization(epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i))(\n",
        "        attention_output + ffn_output)\n",
        "    return sequence_output\n",
        "\n",
        "\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc\n",
        "\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    def train_step(self, inputs):\n",
        "        if len(inputs) == 3:\n",
        "            features, labels, sample_weight = inputs\n",
        "        else:\n",
        "            features, labels = inputs\n",
        "            sample_weight = None\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(features, training=True)\n",
        "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]\n",
        "\n",
        "\n",
        "def create_masked_language_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "\n",
        "    word_embeddings = layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\")(\n",
        "        inputs)\n",
        "    position_embeddings = layers.Embedding(input_dim=config.MAX_LEN,\n",
        "                                           output_dim=config.EMBED_DIM,\n",
        "                                           weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
        "                                           name=\"position_embedding\")(\n",
        "        tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
        "    embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "    encoder_output = embeddings\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        encoder_output = Transformer_block(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
        "        encoder_output)\n",
        "    \n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "    return mlm_model\n",
        "\n",
        "\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}\n",
        "\n",
        "\n",
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.k = top_k\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
        "\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        return id2token[id]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        prediction = self.model.predict(self.sample_tokens)\n",
        "\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
        "        masked_index = masked_index[1]\n",
        "        mask_prediction = prediction[0][masked_index]\n",
        "\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
        "        values = mask_prediction[0][top_indices]\n",
        "\n",
        "        for i in range(len(top_indices)):\n",
        "            p = top_indices[i]\n",
        "            v = values[i]\n",
        "            tokens = np.copy(sample_tokens[0])\n",
        "            tokens[masked_index[0]] = p\n",
        "            result = {\n",
        "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
        "                \"prediction\": self.decode(tokens),\n",
        "                \"probability\": v,\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "            }\n",
        "            pprint(result)\n",
        "\n",
        "\n",
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " word_embedding (Embedding)     (None, 256, 128)     3840000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 256, 128)    0           ['word_embedding[0][0]']         \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " encoder_0/multiheadattention (  (None, 256, 128)    66048       ['tf.__operators__.add[0][0]',   \n",
            " MultiHeadAttention)                                              'tf.__operators__.add[0][0]',   \n",
            "                                                                  'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " encoder_0/att_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/multiheadattention[0]\n",
            " )                                                               [0]']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 256, 128)    0           ['tf.__operators__.add[0][0]',   \n",
            " mbda)                                                            'encoder_0/att_dropout[0][0]']  \n",
            "                                                                                                  \n",
            " encoder_0/att_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_1[0][0]'] \n",
            " on (LayerNormalization)                                                                          \n",
            "                                                                                                  \n",
            " encoder_0/ffn (Sequential)     (None, 256, 128)     33024       ['encoder_0/att_layernormalizatio\n",
            "                                                                 n[0][0]']                        \n",
            "                                                                                                  \n",
            " encoder_0/ffn_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/ffn[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 256, 128)    0           ['encoder_0/att_layernormalizatio\n",
            " mbda)                                                           n[0][0]',                        \n",
            "                                                                  'encoder_0/ffn_dropout[0][0]']  \n",
            "                                                                                                  \n",
            " encoder_0/ffn_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_2[0][0]'] \n",
            " on (LayerNormalization)                                                                          \n",
            "                                                                                                  \n",
            " mlm_cls (Dense)                (None, 256, 30000)   3870000     ['encoder_0/ffn_layernormalizatio\n",
            "                                                                 n[0][0]']                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,809,584\n",
            "Trainable params: 7,809,584\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRXMgl_-xR1V"
      },
      "source": [
        "## Train and Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x79Vy84xR1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548b6d8f-4c03-44c4-d557-4aa14e3b9b8b"
      },
      "source": [
        "%%time\n",
        "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
        "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 6.9841{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'i',\n",
            " 'prediction': 'i have watched this i and it was awesome',\n",
            " 'probability': 0.06641625}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'this',\n",
            " 'prediction': 'i have watched this this and it was awesome',\n",
            " 'probability': 0.059429348}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.033233643}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.029986538}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'it',\n",
            " 'prediction': 'i have watched this it and it was awesome',\n",
            " 'probability': 0.022975946}\n",
            "1563/1563 [==============================] - 384s 243ms/step - loss: 6.9841\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 6.3659{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.3307121}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.20033048}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.025227653}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.023187483}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'show',\n",
            " 'prediction': 'i have watched this show and it was awesome',\n",
            " 'probability': 0.011646813}\n",
            "1563/1563 [==============================] - 380s 243ms/step - loss: 6.3659\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 5.5065{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.58196485}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.13493286}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'show',\n",
            " 'prediction': 'i have watched this show and it was awesome',\n",
            " 'probability': 0.014876597}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.012506193}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'fan',\n",
            " 'prediction': 'i have watched this fan and it was awesome',\n",
            " 'probability': 0.009802288}\n",
            "1563/1563 [==============================] - 381s 244ms/step - loss: 5.5065\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 4.9771{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.48187122}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.17285833}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'dvd',\n",
            " 'prediction': 'i have watched this dvd and it was awesome',\n",
            " 'probability': 0.033915758}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'show',\n",
            " 'prediction': 'i have watched this show and it was awesome',\n",
            " 'probability': 0.022263635}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.012656206}\n",
            "1563/1563 [==============================] - 381s 244ms/step - loss: 4.9771\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 4.6524{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.55106354}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.17152716}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'show',\n",
            " 'prediction': 'i have watched this show and it was awesome',\n",
            " 'probability': 0.02416609}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'dvd',\n",
            " 'prediction': 'i have watched this dvd and it was awesome',\n",
            " 'probability': 0.020845888}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'crap',\n",
            " 'prediction': 'i have watched this crap and it was awesome',\n",
            " 'probability': 0.011831451}\n",
            "1563/1563 [==============================] - 381s 244ms/step - loss: 4.6524\n",
            "CPU times: user 4min 48s, sys: 1min 2s, total: 5min 50s\n",
            "Wall time: 32min 47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USJFONVNxR1Y"
      },
      "source": [
        "## Fine-tune a sentiment classification model\n",
        "\n",
        "We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n",
        "To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n",
        "pretrained BERT features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIFHhn5JxR1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4de993-3c39-4540-c2e0-fdc0cbe20001"
      },
      "source": [
        "# Load pretrained bert model\n",
        "mlm_model = keras.models.load_model(\"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel})\n",
        "pretrained_bert_model = tf.keras.Model(mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output)  # dense 전까지만\n",
        "\n",
        "# Freeze it\n",
        "pretrained_bert_model.trainable = False\n",
        "\n",
        "def create_classifier_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "    sequence_output = pretrained_bert_model(inputs)\n",
        "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
        "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    classifer_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return classifer_model\n",
        "\n",
        "classifer_model = create_classifier_bert_model()\n",
        "classifer_model.summary()\n",
        "\n",
        "# Train the classifier with frozen BERT stage\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")\n",
        "\n",
        "# Unfreeze the BERT model for fine-tuning\n",
        "pretrained_bert_model.trainable = True\n",
        "\n",
        "optimizer = keras.optimizers.Adam()\n",
        "classifer_model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 256)]             0         \n",
            "                                                                 \n",
            " model (Functional)          (None, 256, 128)          3939584   \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,947,905\n",
            "Trainable params: 8,321\n",
            "Non-trainable params: 3,939,584\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 23s 27ms/step - loss: 0.7377 - accuracy: 0.5558 - val_loss: 0.6555 - val_accuracy: 0.6127\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.6660 - accuracy: 0.6050 - val_loss: 0.6387 - val_accuracy: 0.6345\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.6608 - accuracy: 0.6170 - val_loss: 0.6409 - val_accuracy: 0.6337\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.6517 - accuracy: 0.6257 - val_loss: 0.6315 - val_accuracy: 0.6458\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 21s 26ms/step - loss: 0.6466 - accuracy: 0.6265 - val_loss: 0.6616 - val_accuracy: 0.6062\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 41s 49ms/step - loss: 0.4627 - accuracy: 0.7740 - val_loss: 0.3369 - val_accuracy: 0.8533\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 39s 49ms/step - loss: 0.2666 - accuracy: 0.8896 - val_loss: 0.3530 - val_accuracy: 0.8480\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 38s 49ms/step - loss: 0.1336 - accuracy: 0.9490 - val_loss: 0.4342 - val_accuracy: 0.8557\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 39s 49ms/step - loss: 0.0543 - accuracy: 0.9817 - val_loss: 0.5518 - val_accuracy: 0.8464\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 39s 49ms/step - loss: 0.0304 - accuracy: 0.9892 - val_loss: 0.7073 - val_accuracy: 0.8427\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fac2a2be090>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfamRF0XxR1Z"
      },
      "source": [
        "## Create an end-to-end model and evaluate it\n",
        "\n",
        "When you want to deploy a model, it's best if it already includes its preprocessing\n",
        "pipeline, so that you don't have to reimplement the preprocessing logic in your\n",
        "production environment. Let's create an end-to-end model that incorporates\n",
        "the `TextVectorization` layer, and let's evaluate. Our model will accept raw strings\n",
        "as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LCTGb1MxR1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0d2e3e-5950-4488-d3e6-ac0d900517f6"
      },
      "source": [
        "\n",
        "def get_end_to_end(model):\n",
        "    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n",
        "    indices = vectorize_layer(inputs_string)\n",
        "    outputs = model(indices)\n",
        "    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    end_to_end_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return end_to_end_model\n",
        "\n",
        "\n",
        "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
        "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 13s 16ms/step - loss: 0.7073 - accuracy: 0.8427\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7073333859443665, 0.8427199721336365]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm3m9DmEyAtf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}